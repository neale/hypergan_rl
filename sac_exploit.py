import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
# from torch.distributions import Normal
from distributions import TanhNormal
from sac import Replay

i=0
LOG_STD_MIN = -20
LOG_STD_MAX = 2
EPS = 1e-6

def copy_tensor(x):
    return x.clone().detach().cpu()

def danger_mask(x):
    mask = torch.isnan(x) + torch.isinf(x)
    mask = torch.sum(mask, dim=1) > 0
    return mask

def fanin_init(tensor):
    size = tensor.size()
    if len(size) == 2:
        fan_in = size[0]
    elif len(size) > 2:
        fan_in = np.prod(size[1:])
    else:
        raise Exception("Shape must be have dimension at least 2.")
    bound = 1. / np.sqrt(fan_in)
    return tensor.data.uniform_(-bound, bound)


def fanin_init_weights_like(tensor):
    size = tensor.size()
    if len(size) == 2:
        fan_in = size[0]
    elif len(size) > 2:
        fan_in = np.prod(size[1:])
    else:
        raise Exception("Shape must be have dimension at least 2.")
    bound = 1. / np.sqrt(fan_in)
    new_tensor = FloatTensor(tensor.size())
    new_tensor.uniform_(-bound, bound)
    return new_tensor

def init_weights(layer):
    nn.init.orthogonal_(layer.weight)
    nn.init.constant_(layer.bias, 0)


def init_weights_rlkit(layer):
    init_w = 1e-3
    nn.init.uniform_(layer.weight, -init_w, init_w)
    nn.init.uniform_(layer.bias, -init_w, init_w) 


class ParallelLinear_rlkit(nn.Module):
    def __init__(self, n_in, n_out, ensemble_size, final_layer=False):
        super().__init__()

        weights = []
        biases = []
        for j in range(ensemble_size):
            weight = torch.Tensor(n_in, n_out).float()
            bias = torch.Tensor(1, n_out).float()
            if final_layer:
                if j == 0:
                    nn.init.uniform_(weight, -3e-3, 3e-3)  # mean
                    nn.init.uniform_(bias, -3e-3, 3e-3)  # mean
                if j == 1:
                    nn.init.uniform_(weight, -1e-3, 1e-3) # log std
                    nn.init.uniform_(bias, -1e-3, 1e-3)  # log std
            else:
                fanin_init(weight)
                bias.fill_(0.1)

            weights.append(weight)
            biases.append(bias)

        weights = torch.stack(weights)
        biases = torch.stack(biases)

        self.weights = nn.Parameter(weights)
        self.biases = nn.Parameter(biases)

    def forward(self, inp):
        op = torch.baddbmm(self.biases, inp, self.weights)
        return op


class ActionValueFunction_rlkit(nn.Module):
    def __init__(self, d_state, d_action, n_hidden):
        super().__init__()
        self.layers = nn.Sequential(ParallelLinear_rlkit(d_state + d_action, n_hidden, ensemble_size=2),
                                    nn.ReLU(),
                                    ParallelLinear_rlkit(n_hidden, n_hidden, ensemble_size=2),
                                    nn.ReLU(),
                                    ParallelLinear_rlkit(n_hidden, 1, ensemble_size=2, final_layer=True))

    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)
        x = x.unsqueeze(0).repeat(2, 1, 1)
        y1, y2 = self.layers(x)
        return y1, y2


class StateValueFunction_rlkit(nn.Module):
    def __init__(self, d_state, n_hidden):
        super().__init__()
        init_w = 3e-3
        one = nn.Linear(d_state, n_hidden)
        fanin_init(one.weight)
        one.bias.data.fill_(0.1)
        
        two = nn.Linear(n_hidden, n_hidden)
        fanin_init(two.weight)
        two.bias.data.fill_(0.1)
        
        three = nn.Linear(n_hidden, 1)
        three.weight.data.uniform_(-init_w, init_w)
        three.bias.data.uniform_(-init_w, init_w)

        self.layers = nn.Sequential(one,
                                    nn.ReLU(),
                                    two,
                                    nn.ReLU(),
                                    three)

    def forward(self, state):
        result = self.layers(state)
        return result


class TanhGaussianPolicy(nn.Module):
    def __init__(self, d_state, d_action, n_hidden):
        super().__init__()

        one = nn.Linear(d_state, n_hidden)
        one.bias.data.fill_(0.1)
        fanin_init(one.weight)

        two = nn.Linear(n_hidden, n_hidden)
        two.bias.data.fill_(0.1)
        fanin_init(two.weight)

        three = nn.Linear(n_hidden, 2 * d_action)
        init_weights_rlkit(three)

        self.layers = nn.Sequential(one,
                                    nn.ReLU(),
                                    two,
                                    nn.ReLU(),
                                    three)

    def forward(self, state):
        y = self.layers(state)
        mu, log_std = torch.split(y, y.size(1) // 2, dim=1)

        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)
        std = torch.exp(log_std)

        tanh_normal = TanhNormal(mu, std)
        pi, pre_tanh_value = tanh_normal.rsample(return_pretanh_value=True)
        logp_pi = tanh_normal.log_prob(pi, pre_tanh_value=pre_tanh_value)
        logp_pi = logp_pi.sum(dim=1, keepdim=True)

        return pi, logp_pi, mu, log_std

""" changed to take a replay buffer as input instead of taking its own """
class SAC(nn.Module):
    def __init__(self, d_state, d_action, replay_size, n_updates, n_hidden, replay_buffer=None):
        super().__init__()
        self.d_state = d_state
        self.d_action = d_action
        self.gamma = .99
        self.tau = .005
        self.alpha = 1.
        self.reward_scale = 5.
        lr = 3e-4
        batch_size=256
        n_hidden=256
        
        self.replay = replay_buffer
        if self.replay is None:
            self.replay =Replay(d_state=d_state, d_action=d_action, size=replay_size) 
        self.batch_size = batch_size

        self.n_updates = n_updates

        self.qf = ActionValueFunction_rlkit(self.d_state, d_action, n_hidden)
        self.qf_optim = Adam(self.qf.parameters(), lr=lr)

        self.vf = StateValueFunction_rlkit(self.d_state, n_hidden)
        self.vf_target = StateValueFunction_rlkit(self.d_state, n_hidden)
        self.vf_optim = Adam(self.vf.parameters(), lr=lr)
        for target_param, param in zip(self.vf_target.parameters(), self.vf.parameters()):
            target_param.data.copy_(param.data)

        self.policy = TanhGaussianPolicy(self.d_state, d_action, n_hidden)
        self.policy_optim = Adam(self.policy.parameters(), lr=lr)

        self.grad_clip = 5
        self.normalizer = None

    @property
    def device(self):
        return next(self.parameters()).device

    def set_batch_size(self, batch_size):
        self.batch_size = batch_size

    def setup_normalizer(self, normalizer):
        self.normalizer = normalizer
        self.replay.setup_normalizer(normalizer)

    def __call__(self, states, eval=False):
        #if self.normalizer is not None:
        #    states = self.normalizer.normalize_states(states)
        pi, _, mu, _ = self.policy(states)
        return mu if eval else pi

    def get_state_value(self, state):
        if self.normalizer is not None:
            state = self.normalizer.normalize_states(state)
        return self.vf(state)

    def reset_replay(self):
        self.replay.clear()

    def update(self, sample=None):
        global i
        if sample is None:
            sample = self.replay.sample(self.batch_size)
        states, actions, rewards, next_states, masks = [s.to(self.device) for s in sample]

        q1, q2 = self.qf(states, actions) # line 115, 116
        pi, logp_pi, mu, log_std = self.policy(states) # line 119
        q1_pi, q2_pi = self.qf(states, pi) # 152, 153
        v_pred = self.vf(states) # 117

        # target value network
        v_target = self.vf_target(next_states) # 143

        # min double-Q:
        min_q_pi = torch.min(q1_pi, q2_pi) # 151

        # targets for Q and V regression
        q_target = self.reward_scale * rewards + self.gamma * masks * v_target # 144 masks ?= (1-terminals)
        v_backup = min_q_pi - self.alpha * logp_pi # 155

        # policy losses
        pi_loss = torch.mean(self.alpha * logp_pi - min_q_pi) # 179
        pi_loss += 0.001 * mu.pow(2).mean()
        pi_loss += 0.001 * log_std.pow(2).mean()
        # pre_activation_weight=0 so disregard that

        # QF Loss
        q1_loss = F.mse_loss(q1, q_target.detach()) # 145
        q2_loss = F.mse_loss(q2, q_target.detach()) # 146

        # VF Loss
        v_loss = F.mse_loss(v_pred, v_backup.detach()) # 156
        q_loss = q1_loss + q2_loss

        self.policy_optim.zero_grad()
        pi_loss.backward()
        self.policy_optim.step()

        self.qf_optim.zero_grad()
        q_loss.backward()
        self.qf_optim.step()

        self.vf_optim.zero_grad()
        v_loss.backward()
        self.vf_optim.step()

        for target_param, param in zip(self.vf_target.parameters(), self.vf.parameters()):
            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)

        return v_loss.item(), q1_loss.item(), q2_loss.item(), pi_loss.item()

    def episode(self, env, warm_up=False, train=True, verbosity=0, _log=None):
        ep_returns = 0
        ep_length = 0
        states = env.reset()
        done = False
        while not done:
            if warm_up:
                actions = env.action_space.sample()
                actions = torch.from_numpy(actions)
            else:
                with torch.no_grad():
                    actions = self(states)

            next_states, rewards, done, _ = env.step(actions)
            self.replay.add(states, actions, rewards, next_states)
            ep_returns += rewards.squeeze().detach().data.cpu().numpy()
            ep_length += 1

            states = next_states
            self.update()

        #if train:
        #    if not warm_up:
        #for _ in range(self.n_updates * ep_length):

        return ep_returns
